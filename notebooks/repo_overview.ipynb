{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ear‑vision‑ml Repository Overview\n",
    "\n",
    "This notebook provides a step‑by‑step walkthrough of the core functionality of the **ear‑vision‑ml** repository with runnable examples. Each section corresponds to a key module in the codebase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The repository implements a pipeline for training and evaluating ear‑vision models, handling data loading, model configuration, training callbacks, and exporting models to various formats. The high‑level architecture is based on a Dependency Injection (DI) container that wires services together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Configuration (`config_utils.py` & `constants.py`)\n",
    "\n",
    "Configuration files are validated against a JSON schema. The helper functions load a YAML/JSON config and expose constants such as default paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import config_utils, constants\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Load a sample configuration (the repository ships a default example)\n",
    "# We use OmegaConf directly as config_utils provides safe accessors but not a loader\n",
    "try:\n",
    "    sample_cfg = OmegaConf.load('configs/sample_config.yaml')\n",
    "    print('Loaded config keys:', list(sample_cfg.keys()))\n",
    "except FileNotFoundError:\n",
    "    print('Sample config not found, creating a dummy one.')\n",
    "    sample_cfg = OmegaConf.create({'data': {'batch_size': 32}})\n",
    "    print('Created dummy config keys:', list(sample_cfg.keys()))\n",
    "\n",
    "print('Shuffle buffer size from constants:', constants.SHUFFLE_BUFFER_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dependency Injection (`di.py`)\n",
    "\n",
    "The DI container registers services (e.g., data loaders, model factories) and resolves them on demand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.di import Container\n",
    "\n",
    "container = Container()\n",
    "# Resolve the dataset loader service\n",
    "# Note: In a real app, we would register services first. Here we check if it resolves or raises error.\n",
    "try:\n",
    "    loader = container.resolve('dataset_loader')\n",
    "    print('Resolved service type:', type(loader))\n",
    "except Exception as e:\n",
    "    print(f'Service resolution failed as expected (no config loaded): {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading (`dataset_loader.py`)\n",
    "\n",
    "The repository provides flexible dataset loading strategies (e.g., TFRecord, synthetic). Below we load a tiny synthetic dataset and visualise a sample image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.data.dataset_loader import SyntheticDataLoader\n",
    "from omegaconf import OmegaConf\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    plt = None\n",
    "\n",
    "# Create a config for the synthetic loader\n",
    "cfg = OmegaConf.create({\n",
    "    'data': {\n",
    "        'dataset': {\n",
    "            'image_size': [64, 64],\n",
    "            'num_classes': 3,\n",
    "            'batch_size': 2\n",
    "        }\n",
    "    },\n",
    "    'model': {\n",
    "        'num_classes': 3\n",
    "    },\n",
    "    'task': {\n",
    "        'name': 'classification'\n",
    "    }\n",
    "})\n",
    "\n",
    "loader = SyntheticDataLoader()\n",
    "ds = loader.load_train(cfg)\n",
    "\n",
    "# Take one batch\n",
    "for images, labels in ds.take(1):\n",
    "    print('Dataset batch shape:', images.shape, 'Labels batch shape:', labels.shape)\n",
    "    if plt:\n",
    "        plt.imshow(images[0].numpy().astype('float32'), cmap='gray')\n",
    "        plt.title(f'Label: {labels[0]}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Callbacks (`callbacks.py`)\n",
    "\n",
    "Callbacks hook into the training loop to log metrics, save checkpoints, or adjust learning rates. The example demonstrates a simple progress logger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.training.callbacks import WarmUpLearningRate\n",
    "\n",
    "# Initialize the callback\n",
    "callback = WarmUpLearningRate(target_lr=0.001, warmup_epochs=3)\n",
    "print('Initialized WarmUpLearningRate callback')\n",
    "\n",
    "# Simulate epoch begin to see if it runs\n",
    "try:\n",
    "    callback.on_epoch_begin(0)\n",
    "    print('Callback on_epoch_begin executed successfully')\n",
    "except Exception as e:\n",
    "    print(f'Callback execution failed (expected if model not attached): {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exporters (`exporter.py`)\n",
    "\n",
    "Models can be exported to CoreML, ONNX, etc. The snippet below creates a dummy Keras model and exports it to CoreML. It gracefully skips if TensorFlow is not available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76da798",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    tf = None\n",
    "from src.core.export.exporter import CoreMLExporter\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "if tf:\n",
    "    # Build a tiny model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(64, 64, 3)),\n",
    "        tf.keras.layers.Conv2D(8, 3, activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    \n",
    "    # Prepare config for export\n",
    "    export_cfg = OmegaConf.create({\n",
    "        'task': {'name': 'classification'},\n",
    "        'data': {\n",
    "            'dataset': {\n",
    "                'image_size': [64, 64],\n",
    "                'class_names': ['class_0', 'class_1']\n",
    "            }\n",
    "        },\n",
    "        'model': {'num_classes': 2},\n",
    "        'export': {\n",
    "            'export': {\n",
    "                'coreml': {\n",
    "                    'enabled': True,\n",
    "                    'quantize': False\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    exporter = CoreMLExporter()\n",
    "    output_dir = Path('exported_models')\n",
    "    \n",
    "    # Clean up previous run\n",
    "    if output_dir.exists():\n",
    "        shutil.rmtree(output_dir)\n",
    "        \n",
    "    coreml_path = exporter.export(model, output_dir=output_dir, cfg=export_cfg)\n",
    "    if coreml_path:\n",
    "        print('CoreML model saved to', coreml_path)\n",
    "    else:\n",
    "        print('CoreML export skipped or failed (check logs).')\n",
    "else:\n",
    "    print('TensorFlow not installed; skipping export example.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Utilities & Logging (`logging_utils.py`)\n",
    "\n",
    "A consistent logging configuration is provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.logging_utils import get_logger\n",
    "logger = get_logger(__name__)\n",
    "logger.info('Logging from the notebook works!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Running Scripts Overview\n",
    "\n",
    "Key entry‑point scripts include `run_otoscopic_baselines.sh` (full training pipeline) and `scripts/generate_fixtures.py` (data generation). They glue together the components demonstrated above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Testing Overview\n",
    "\n",
    "The repository ships unit and integration tests under the `tests/` directory. Run them with:\n",
    "```bash\n",
    "pytest -vv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion & Further Resources\n",
    "\n",
    "You now have a high‑level understanding of the main modules and how to use them. For deeper dives, explore the `src/core/` package, the `scripts/` folder, and the extensive test suite.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
