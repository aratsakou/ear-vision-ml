epochs: 100
learning_rate: 0.001
optimizer: adam
batch_size: 32

# TensorBoard logging
tensorboard:
  enabled: true
  profile_batch: "0"  # Disable profiling by default
  histogram_freq: 1
  write_graph: true
  write_images: false
  update_freq: "epoch"

# Model checkpointing
checkpoint:
  enabled: true
  monitor: val_loss
  mode: min
  save_freq: 5  # Save every N epochs

# Early stopping
early_stopping:
  enabled: true
  monitor: "val_loss"
  patience: 10
  min_delta: 0.001
  restore_best_weights: true

# Learning rate scheduling
lr_schedule:
  enabled: true
  type: "reduce_on_plateau"  # Options: reduce_on_plateau, cosine_annealing, exponential_decay
  monitor: "val_loss"
  factor: 0.5
  patience: 5
  min_lr: 1e-6
  # For exponential_decay:
  decay_rate: 0.96
  decay_steps: 10

# Warm-up
warmup:
  enabled: false
  epochs: 5

# Regularization (L1/L2)
regularizer:
  enabled: false
  l1: 0.0
  l2: 0.0001

# Class Weighting (for unbalanced datasets)
class_weights: false

# Loss function configuration (optional override)
# loss:
#   type: "focal"
#   alpha: 0.25
#   gamma: 2.0

# CSV logging
csv_logger:
  enabled: true

# Mixed precision training (TF 2.4+)
mixed_precision:
  enabled: false
  policy: mixed_float16  # or mixed_bfloat16 for TPUs

# Gradient clipping
gradient_clip:
  enabled: false
  clip_norm: 1.0
  clip_value: null

# Hyperparameter Tuning
tuning:
  enabled: false
  max_trials: 20
  max_parallel_trials: 2
  study_spec:
    metrics:
      - metric_id: "val_accuracy"
        goal: "MAXIMIZE"
    parameters:
      - parameter_id: "training.learning_rate"
        double_value_spec:
          min_value: 1e-4
          max_value: 1e-2
        scale_type: "UNIT_LOG_SCALE"
      - parameter_id: "model.dropout"
        double_value_spec:
          min_value: 0.0
          max_value: 0.5
          step_size: 0.1
